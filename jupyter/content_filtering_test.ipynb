{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Filtering Tests\n",
    "\n",
    "This notebook tests the enhanced content filtering functionality of llm-browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Add the parent directory to the path so we can import from the llm_browser package\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the enhanced utility functions\n",
    "from llm_browser.utils.html import (\n",
    "    extract_main_content,\n",
    "    html_to_markdown,\n",
    "    extract_navigation,\n",
    "    format_navigation_as_markdown,\n",
    "    find_github_source_link\n",
    ")\n",
    "from llm_browser.utils.url import github_url_to_raw, is_github_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_html(url: str) -> Optional[str]:\n",
    "    \"\"\"Fetch HTML content from a URL\"\"\"\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    \n",
    "    try:\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(url, headers=headers, timeout=30.0, follow_redirects=True)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def print_section(title, content, max_length=1000):\n",
    "    \"\"\"Print a section of content with optional truncation\"\"\"\n",
    "    print(f\"\\n{'=' * 80}\\n{title}\\n{'=' * 80}\\n\")\n",
    "    if len(content) > max_length:\n",
    "        print(f\"{content[:max_length]}...\\n\\n[truncated - {len(content)} characters total]\")\n",
    "    else:\n",
    "        print(content)\n",
    "        \n",
    "async def test_content_extraction(url: str, content_priorities=[\"auto\", \"main\", \"article\", \"largest\", \"dense\"]):\n",
    "    \"\"\"Test content extraction with different priority modes\"\"\"\n",
    "    html_content = await fetch_html(url)\n",
    "    if not html_content:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n\\n{'#' * 80}\\nTESTING URL: {url}\\n{'#' * 80}\\n\")\n",
    "    \n",
    "    # Check if this is a GitHub URL\n",
    "    if is_github_url(url):\n",
    "        raw_url = github_url_to_raw(url)\n",
    "        print_section(\"GitHub Raw URL\", raw_url)\n",
    "        \n",
    "    # Check for GitHub source links\n",
    "    github_link = find_github_source_link(html_content)\n",
    "    if github_link:\n",
    "        print_section(\"Found GitHub Source Link\", \n",
    "                      f\"Text: {github_link['text']}\\nURL: {github_link['href']}\\nRaw URL: {github_link.get('raw_url')}\")\n",
    "    \n",
    "    # Extract and print navigation\n",
    "    nav_sections = extract_navigation(html_content)\n",
    "    if nav_sections:\n",
    "        nav_markdown = format_navigation_as_markdown(nav_sections)\n",
    "        print_section(\"Navigation Structure\", nav_markdown, 2000)\n",
    "    \n",
    "    # Test different content extraction strategies\n",
    "    for priority in content_priorities:\n",
    "        main_content_html = extract_main_content(html_content, content_priority=priority)\n",
    "        main_content_text = BeautifulSoup(main_content_html, \"html.parser\").get_text(strip=True)\n",
    "        print_section(f\"Extracted Content ({priority.upper()} priority)\", \n",
    "                      f\"[Characters: {len(main_content_html)} HTML / {len(main_content_text)} text]\\n\\n{main_content_text[:500]}...\",\n",
    "                      800)\n",
    "        \n",
    "        # Convert to markdown with different options\n",
    "        markdown_content = html_to_markdown(\n",
    "            html_content, \n",
    "            content_priority=priority,\n",
    "            strip_comments=True,\n",
    "            strip_ads=True\n",
    "        )\n",
    "        print_section(f\"Markdown ({priority.upper()} priority)\", markdown_content[:1000] + \"...\", 1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Various Website Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a documentation page with GitHub edit link\n",
    "await test_content_extraction(\"https://svelte.dev/docs/introduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a GitHub repository page directly\n",
    "await test_content_extraction(\"https://github.com/sveltejs/kit/blob/main/documentation/docs/10-getting-started/10-introduction.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a page with <main> tag\n",
    "await test_content_extraction(\"https://web.dev/articles/semantic-html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a page with articles and comments\n",
    "await test_content_extraction(\"https://news.ycombinator.com/item?id=32556068\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a page with navigation and sidebar\n",
    "await test_content_extraction(\"https://react.dev/learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Content Density Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_element_density(url: str, top_n=10):\n",
    "    \"\"\"Analyze the content density of elements on a page\"\"\"\n",
    "    html_content = await fetch_html(url)\n",
    "    if not html_content:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # Find substantial elements\n",
    "    elements = []\n",
    "    for tag_name in [\"div\", \"section\", \"article\", \"main\", \"table\", \"aside\"]:\n",
    "        for element in soup.find_all(tag_name):\n",
    "            text = element.get_text(strip=True)\n",
    "            text_length = len(text)\n",
    "            if text_length < 100:  # Skip tiny elements\n",
    "                continue\n",
    "                \n",
    "            html_length = len(str(element))\n",
    "            link_count = len(element.find_all(\"a\"))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            text_density = text_length / html_length if html_length > 0 else 0\n",
    "            text_to_link_ratio = text_length / link_count if link_count > 0 else text_length * 0.5\n",
    "            \n",
    "            # Store with different scoring formulas\n",
    "            size_score = (text_length * 0.6) + (text_density * 20) + (text_to_link_ratio * 0.1)\n",
    "            density_score = (text_density * 50) + (text_to_link_ratio * 0.2) + (text_length * 0.01)\n",
    "            balanced_score = (text_length * 0.3) + (text_density * 30) + (text_to_link_ratio * 0.15)\n",
    "            \n",
    "            identifier = f\"{tag_name} - {element.get('id', '')} {' '.join(element.get('class', []))}\"[:50]\n",
    "            elements.append({\n",
    "                \"element\": element,\n",
    "                \"identifier\": identifier,\n",
    "                \"text_length\": text_length,\n",
    "                \"html_length\": html_length,\n",
    "                \"link_count\": link_count,\n",
    "                \"text_density\": text_density,\n",
    "                \"text_to_link_ratio\": text_to_link_ratio,\n",
    "                \"size_score\": size_score,\n",
    "                \"density_score\": density_score,\n",
    "                \"balanced_score\": balanced_score\n",
    "            })\n",
    "    \n",
    "    print(f\"Analyzed {len(elements)} elements on {url}\\n\")\n",
    "    \n",
    "    # Report top elements by each score\n",
    "    for score_type in [\"size_score\", \"density_score\", \"balanced_score\"]:\n",
    "        print(f\"\\nTop {top_n} elements by {score_type}:\")\n",
    "        top_elements = sorted(elements, key=lambda x: x[score_type], reverse=True)[:top_n]\n",
    "        \n",
    "        print(f\"{'Rank':<5}{'Element':<52}{'Text Len':<10}{'Density':<10}{'Link Ratio':<12}{'Score':<10}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for i, element in enumerate(top_elements, 1):\n",
    "            print(f\"{i:<5}{element['identifier']:<52}{element['text_length']:<10}{element['text_density']:.3f}   {element['text_to_link_ratio']:<12.1f}{element[score_type]:<10.1f}\")\n",
    "            \n",
    "        # Show text excerpt from top element\n",
    "        top_text = top_elements[0]['element'].get_text(strip=True)\n",
    "        print(f\"\\nExcerpt from top element:\\n{top_text[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze density on different page types\n",
    "await analyze_element_density(\"https://news.ycombinator.com/item?id=32556068\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await analyze_element_density(\"https://web.dev/articles/semantic-html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GitHub Raw Content Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_github_raw(url: str):\n",
    "    \"\"\"Test GitHub raw content detection and conversion\"\"\"\n",
    "    # Fetch the original URL\n",
    "    html_content = await fetch_html(url)\n",
    "    if not html_content:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Testing GitHub integration for: {url}\\n\")\n",
    "    \n",
    "    # If this is a direct GitHub URL, convert it\n",
    "    if is_github_url(url):\n",
    "        raw_url = github_url_to_raw(url)\n",
    "        print(f\"Direct conversion to raw URL:\\n{raw_url}\\n\")\n",
    "        \n",
    "        # Fetch the raw content\n",
    "        raw_content = await fetch_html(raw_url)\n",
    "        if raw_content:\n",
    "            print_section(\"Raw content sample\", raw_content[:1000])\n",
    "    \n",
    "    # Check for GitHub source links in the page\n",
    "    github_link = find_github_source_link(html_content)\n",
    "    if github_link:\n",
    "        print(f\"\\nFound GitHub source link in page:\\n\")\n",
    "        print(f\"Link text: {github_link['text']}\")\n",
    "        print(f\"Link URL: {github_link['href']}\")\n",
    "        \n",
    "        if github_link.get('raw_url'):\n",
    "            print(f\"Converted raw URL: {github_link['raw_url']}\")\n",
    "            \n",
    "            # Fetch and show the raw content\n",
    "            raw_content = await fetch_html(github_link['raw_url'])\n",
    "            if raw_content:\n",
    "                print_section(\"Source raw content sample\", raw_content[:1000])\n",
    "    else:\n",
    "        print(\"No GitHub source link found in this page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different GitHub scenarios\n",
    "await test_github_raw(\"https://github.com/sveltejs/kit/blob/main/documentation/docs/10-getting-started/10-introduction.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await test_github_raw(\"https://svelte.dev/docs/introduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Complete Content Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_url(url: str, prefer_raw=True, include_navigation=True, \n",
    "                      content_priority=\"auto\", strip_comments=True, strip_ads=True):\n",
    "    \"\"\"Process a URL with the complete pipeline\"\"\"\n",
    "    print(f\"Processing URL: {url}\")\n",
    "    print(f\"Options: prefer_raw={prefer_raw}, include_navigation={include_navigation}, \")\n",
    "    print(f\"         content_priority={content_priority}, strip_comments={strip_comments}, strip_ads={strip_ads}\\n\")\n",
    "    \n",
    "    # Handle direct GitHub URLs\n",
    "    if is_github_url(url) and prefer_raw:\n",
    "        raw_url = github_url_to_raw(url)\n",
    "        print(f\"Direct GitHub URL detected, using raw URL: {raw_url}\")\n",
    "        raw_content = await fetch_html(raw_url)\n",
    "        if raw_content:\n",
    "            # Extract filename from URL\n",
    "            filename = url.split(\"/\")[-1].split(\"?\")[0]\n",
    "            content = f\"# {filename}\\n\\n{raw_content}\"\n",
    "            print_section(\"Processed content (from direct GitHub URL)\", content[:2000])\n",
    "            return\n",
    "    \n",
    "    # For non-GitHub URLs or if raw fetch failed\n",
    "    html_content = await fetch_html(url)\n",
    "    if not html_content:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return\n",
    "        \n",
    "    # Check for GitHub source links\n",
    "    if prefer_raw:\n",
    "        github_link = find_github_source_link(html_content)\n",
    "        if github_link and github_link.get(\"raw_url\"):\n",
    "            raw_url = github_link[\"raw_url\"]\n",
    "            print(f\"Found GitHub source link, using raw URL: {raw_url}\")\n",
    "            raw_content = await fetch_html(raw_url)\n",
    "            \n",
    "            if raw_content:\n",
    "                content = raw_content\n",
    "                \n",
    "                # Extract navigation if requested\n",
    "                if include_navigation:\n",
    "                    nav_sections = extract_navigation(html_content)\n",
    "                    if nav_sections:\n",
    "                        nav_markdown = format_navigation_as_markdown(nav_sections)\n",
    "                        if nav_markdown:\n",
    "                            content = f\"{nav_markdown}\\n---\\n\\n{content}\"\n",
    "                \n",
    "                # Add title if needed\n",
    "                if not content.startswith(\"# \"):\n",
    "                    from llm_browser.utils.html import extract_title\n",
    "                    title = extract_title(html_content) or raw_url.split(\"/\")[-1]\n",
    "                    content = f\"# {title}\\n\\n{content}\"\n",
    "                    \n",
    "                print_section(\"Processed content (from GitHub source link)\", content[:2000])\n",
    "                return\n",
    "    \n",
    "    # If no GitHub raw content, process with HTML to markdown\n",
    "    title = extract_title(html_content) or url\n",
    "    \n",
    "    # Convert to markdown with filtering options\n",
    "    markdown_content = html_to_markdown(\n",
    "        html_content,\n",
    "        content_priority=content_priority,\n",
    "        strip_comments=strip_comments,\n",
    "        strip_ads=strip_ads\n",
    "    )\n",
    "    \n",
    "    # Extract navigation if requested\n",
    "    if include_navigation:\n",
    "        nav_sections = extract_navigation(html_content)\n",
    "        if nav_sections:\n",
    "            nav_markdown = format_navigation_as_markdown(nav_sections)\n",
    "            if nav_markdown:\n",
    "                markdown_content = f\"{nav_markdown}\\n---\\n\\n{markdown_content}\"\n",
    "    \n",
    "    # Add title\n",
    "    content = f\"# {title}\\n\\n{markdown_content}\"\n",
    "    \n",
    "    print_section(\"Processed content (from HTML)\", content[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different URLs with our processing pipeline\n",
    "await process_url(\"https://svelte.dev/docs/introduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_url(\"https://github.com/sveltejs/kit/blob/main/documentation/docs/10-getting-started/10-introduction.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different content extraction strategies\n",
    "await process_url(\"https://web.dev/articles/semantic-html\", content_priority=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_url(\"https://web.dev/articles/semantic-html\", content_priority=\"largest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with navigation disabled\n",
    "await process_url(\"https://react.dev/learn\", include_navigation=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}